{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.datasets import UrbanSound8KDataset, get_data_loaders\n",
    "\n",
    "data_csv='/data/urbansound8k/UrbanSound8K.csv'\n",
    "root_dir='/data/urbansound8k'\n",
    "train_fold=[1,2,3,4,5,6,7,8]\n",
    "val_fold=[10]\n",
    "test_fold=[9]\n",
    "train_loader, val_loader, test_loader = get_data_loaders(data_csv, root_dir, train_fold, val_fold, test_fold, batch_size=32, mode=\"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_labels = []\n",
    "for _, labels in test_loader:\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "label_counts = Counter(all_labels)\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# Fetch the first batch from the train_loader\n",
    "for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "    # Print the shape of the data and labels\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(f\"Data shape: {data.shape}\")  # Should be [batch_size, 128, 84] if correct\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Take the first example in the batch\n",
    "    example_spectrogram = data[0].numpy()\n",
    "    example_label = labels[0].item()\n",
    "    \n",
    "    # Get the corresponding file information from the dataset annotations\n",
    "    annotation_idx = train_loader.dataset.annotations.index[batch_idx * len(data)]\n",
    "    file_name = train_loader.dataset.annotations.iloc[annotation_idx, 0]\n",
    "    fold_number = train_loader.dataset.annotations.iloc[annotation_idx, 5]\n",
    "    \n",
    "    # Print the file name, fold number, and label\n",
    "    print(f\"File: {file_name}\")\n",
    "    print(f\"Fold: {fold_number}\")\n",
    "    print(f\"Label: {example_label}\")\n",
    "    \n",
    "    # Plot the mel-spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(example_spectrogram, aspect='auto', origin='lower')\n",
    "    plt.title(f\"Mel-Spectrogram Example - Label: {example_label}\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Mel Frequency Bands')\n",
    "    \n",
    "    # Save the plot with detailed filename\n",
    "    save_path = f'/home/ilias/projects/adversarial_thesis/data/mel_spectrogram_fold{fold_number}_label{example_label}_{os.path.splitext(file_name)[0]}.png'\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilias/miniconda3/envs/adversarial_thesis/lib/python3.9/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/tmp/ipykernel_357623/3369742589.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/ilias/projects/adversarial_thesis/src/models/baseline_cnn.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselineCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=10240, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.models import BaselineCNN\n",
    "import torch \n",
    "\n",
    "model = BaselineCNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('/home/ilias/projects/adversarial_thesis/src/models/baseline_cnn.pth'))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"/data/urbansound8k/fold10/15544-5-0-8.wav\"\n",
    "\n",
    "output1 = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loops.trainer import test\n",
    "\n",
    "test(test_loader=test_loader, model=model, device = torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO Attack example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import IPython\n",
    "\n",
    "audio_file = \"/data/urbansound8k/fold10/15544-5-0-8.wav\"\n",
    "IPython.display.display(IPython.display.Audio(audio_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "original_audio, sr = librosa.load(audio_file)\n",
    "\n",
    "# Plot the waveform\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(original_audio, sr=sr, ax=ax)\n",
    "\n",
    "# Customize the plot as needed\n",
    "ax.set(title='Waveform of Example Audio File')\n",
    "ax.label_outer()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create noise & plot noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epsilon = 0.3\n",
    "\n",
    "noise = np.random.uniform(\n",
    "            -np.abs(original_audio),  # Minimum noise for each point\n",
    "            np.abs(original_audio)   # Maximum noise for each point\n",
    "        ) * epsilon  # Scale by epsilon\n",
    "\n",
    "# Plot the waveform\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(noise, sr=sr, ax=ax)\n",
    "\n",
    "# Customize the plot as needed\n",
    "ax.set(title='Waveform of Example Audio File')\n",
    "ax.label_outer()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import calculate_snr\n",
    "\n",
    "snr = calculate_snr(original_audio, noise)\n",
    "print(f\"SNR = {snr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbed audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed = original_audio + noise\n",
    "# Plot the waveform\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(perturbed, sr=sr, ax=ax)\n",
    "\n",
    "# Customize the plot as needed\n",
    "ax.set(title='Waveform of perturbed example')\n",
    "ax.label_outer()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(perturbed, rate=22010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_penalty = np.linalg.norm(original_audio - original_audio)\n",
    "l2_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESC-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.utils import extract_mel_spectrogram\n",
    "\n",
    "CATEGORY_MAPPING = {\n",
    "    \"dog\": \"Animals\",\n",
    "    \"rooster\": \"Animals\",\n",
    "    \"pig\": \"Animals\",\n",
    "    \"cow\": \"Animals\",\n",
    "    \"frog\": \"Animals\",\n",
    "    \"cat\": \"Animals\",\n",
    "    \"hen\": \"Animals\",\n",
    "    \"insects\": \"Animals\",\n",
    "    \"sheep\": \"Animals\",\n",
    "    \"crow\": \"Animals\",\n",
    "    \"rain\": 'Natural soundscapes & water sounds',\n",
    "    \"sea_waves\": 'Natural soundscapes & water sounds',\n",
    "    \"crackling_fire\": 'Natural soundscapes & water sounds',\n",
    "    \"crickets\": 'Natural soundscapes & water sounds',\n",
    "    \"chirping_birds\": 'Natural soundscapes & water sounds',\n",
    "    'water_drops': 'Natural soundscapes & water sounds',\n",
    "    \"wind\": 'Natural soundscapes & water sounds',\n",
    "    'pouring_water': 'Natural soundscapes & water sounds',\n",
    "    \"toilet_flush\": 'Natural soundscapes & water sounds',\n",
    "    \"thunderstorm\": 'Natural soundscapes & water sounds',\n",
    "    \"crying baby\": \"Human, non-speech sounds\",\n",
    "    \"sneezing\": \"Human, non-speech sounds\",\n",
    "    \"clapping\": \"Human, non-speech sounds\",\n",
    "    \"breathing\": \"Human, non-speech sounds\",\n",
    "    \"coughing\": \"Human, non-speech sounds\",\n",
    "    \"footsteps\": \"Human, non-speech sounds\",\n",
    "    \"laughing\": \"Human, non-speech sounds\",\n",
    "    \"brushing_teeth\": \"Human, non-speech sounds\",\n",
    "    \"snoring\": \"Human, non-speech sounds\",\n",
    "    \"drinking_sipping\": \"Human, non-speech sounds\",\n",
    "    \"door_wood_knock\": \"Interior/domestic sounds\",\n",
    "    \"mouse_click\": \"Interior/domestic sounds\",\n",
    "    \"keyboard_typing\": \"Interior/domestic sounds\",\n",
    "    \"door_wood_creaks\": \"Interior/domestic sounds\",\n",
    "    \"can_opening\": \"Interior/domestic sounds\",\n",
    "    \"washing_machine\": \"Interior/domestic sounds\",\n",
    "    \"vacuum_cleaner\": \"Interior/domestic sounds\",\n",
    "    \"clock_alarm\": \"Interior/domestic sounds\",\n",
    "    \"clock_tick\": \"Interior/domestic sounds\",\n",
    "    \"glass_breaking\": \"Interior/domestic sounds\",\n",
    "    \"helicopter\": \"Exterior/urban noises\",\n",
    "    \"chainsaw\": \"Exterior/urban noises\",\n",
    "    \"siren\": \"Exterior/urban noises\",\n",
    "    \"car_horn\": \"Exterior/urban noises\",\n",
    "    \"engine\": \"Exterior/urban noises\",\n",
    "    \"train\": \"Exterior/urban noises\",\n",
    "    \"church_bells\": \"Exterior/urban noises\",\n",
    "    \"airplane\": \"Exterior/urban noises\",\n",
    "    \"fireworks\": \"Exterior/urban noises\",\n",
    "    \"hand_saw\": \"Exterior/urban noises\"\n",
    "}\n",
    "\n",
    "class ESC50Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, root_dir, folds, mode='train', transform=None):\n",
    "        \"\"\"\n",
    "        ESC-50 dataset class.\n",
    "\n",
    "        Args:\n",
    "            annotations_file (str): Path to the annotations CSV file.\n",
    "            root_dir (str): Root directory containing audio files.\n",
    "            folds (list): List of fold numbers to include in the dataset.\n",
    "            mode (str): 'train' for mel-spectrograms, 'attack' for waveforms, 'AudioCLIP' for normalized waveforms.\n",
    "            transform (callable, optional): Transformation function for data augmentation.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.folds = folds\n",
    "        self.mode = mode  # 'train', 'attack', or 'AudioCLIP'\n",
    "        self.sr = 22050\n",
    "        self.target_length = 4 * self.sr  # 4 seconds at 22050 Hz\n",
    "        # Filter annotations to include only the specified folds\n",
    "        self.annotations = self.annotations[self.annotations['fold'].isin(self.folds)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (features, label, file path), where features can be a mel-spectrogram or waveform.\n",
    "        \"\"\"\n",
    "        # Get the file path\n",
    "        file_name = self.annotations.iloc[idx, 0]\n",
    "        audio_file_path = os.path.join(self.root_dir, file_name)\n",
    "        \n",
    "        # Load the audio file\n",
    "        audio, sample_rate = librosa.load(audio_file_path, sr=self.sr)\n",
    "        \n",
    "        # Ensure all audio has the same length (4 seconds)\n",
    "        if len(audio) < self.target_length:\n",
    "            padding = self.target_length - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode='constant')\n",
    "        elif len(audio) > self.target_length:\n",
    "            audio = audio[:self.target_length]\n",
    "        \n",
    "        # Get the label and map to higher-level category\n",
    "        category = self.annotations.iloc[idx, 3]\n",
    "        label = CATEGORY_MAPPING.get(category, \"Unknown\")\n",
    "        \n",
    "        # Depending on the mode, return the appropriate features\n",
    "        if self.mode == 'train' or self.mode == \"evaluate\":\n",
    "            features = extract_mel_spectrogram(audio, sample_rate)\n",
    "            if self.transform:\n",
    "                features = self.transform(features)\n",
    "        elif self.mode == 'attack':\n",
    "            features = audio\n",
    "        elif self.mode == 'AudioCLIP':\n",
    "            audio = torch.tensor(audio, dtype=torch.float32)\n",
    "            features = audio\n",
    "                \n",
    "        return features, label, audio_file_path\n",
    "\n",
    "def get_esc50_data_loaders(annotations_file, root_dir, train_folds, val_folds, test_folds, batch_size=32, transform=None, mode=\"train\"):\n",
    "    # Create datasets for training, validation, and testing\n",
    "    train_dataset = ESC50Dataset(annotations_file, root_dir, train_folds, mode=mode, transform=transform)\n",
    "    val_dataset = ESC50Dataset(annotations_file, root_dir, val_folds, mode=mode, transform=transform)\n",
    "    test_dataset = ESC50Dataset(annotations_file, root_dir, test_folds, mode=mode, transform=transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample File Path: /data/ESC-50-master/audio/1-100032-A-0.wav\n",
      "Label (Higher Class): Animals\n",
      "Feature Shape: torch.Size([1, 128, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilias/projects/adversarial_thesis/src/utils/utils.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mel_tensor = torch.tensor(mel_spectrogram_db, dtype=torch.float32).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "annotations_file = \"/data/ESC-50-master/meta/esc50.csv\" \n",
    "audio_root_dir = \"/data/ESC-50-master/audio\"  \n",
    "\n",
    "# Define dataset folds\n",
    "train_folds = [1, 2, 3]  # Example train folds\n",
    "val_folds = [4]          # Example validation fold\n",
    "test_folds = [5]         # Example test fold\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = ESC50Dataset(annotations_file, audio_root_dir, folds=train_folds, mode='train')\n",
    "\n",
    "# Load a single sample\n",
    "features, label, file_path = dataset[0]\n",
    "\n",
    "# Print output\n",
    "print(\"Sample File Path:\", file_path)\n",
    "print(\"Label (Higher Class):\", label)\n",
    "print(\"Feature Shape:\", features.shape if isinstance(features, torch.Tensor) else len(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
